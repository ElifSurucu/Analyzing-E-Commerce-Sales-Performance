{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6899ad425e03acd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Phase 3 Code Challenge\n",
    "\n",
    "This assessment is designed to test your understanding of Module 3 material. It covers:\n",
    "\n",
    "* Logistic Regression\n",
    "* Classification Metrics\n",
    "* Decision Trees\n",
    "\n",
    "_Read the instructions carefully_. You will be asked both to write code and to answer short answer questions.\n",
    "\n",
    "## Short Answer Questions \n",
    "\n",
    "For the short answer questions...\n",
    "\n",
    "* _Use your own words_. It is OK to refer to outside resources when crafting your response, but _do not copy text from another source_.\n",
    "\n",
    "* _Communicate clearly_. We are not grading your writing skills, but you can only receive full credit if your teacher is able to fully understand your response. \n",
    "\n",
    "* _Be concise_. You should be able to answer most short answer questions in a sentence or two. Writing unnecessarily long answers increases the risk of you being unclear or saying something incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c2a2bae912a0e147",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes to import the necessary libraries\n",
    "import pickle, sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\n",
    "from numbers import Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58cbc9e518eda9a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Part 1: Logistic Regression\n",
    "---\n",
    "In this part, you will answer general questions about logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a5eed21ce4450ee7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1) Short Answer: Provide one reason why logistic regression is better than linear regression for modeling a binary target/outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "Logistic regression is more suitable for modeling a binary target because its output is a probability value between 0 and 1, thus providing meaningful results in classification problems. On the other hand, linear regression may give results that are not suitable for binary classification because it produces continuous values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc85e3d7f84c78d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2) Short Answer: Compare logistic regression to another classification model of your choice (e.g. Decision Tree). What is one advantage and one disadvantage logistic regression has when compared with the other model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "The advantage of logistic regression is its speed and lower computational cost on large datasets, and it provides interpretable probability outputs. However, it performs poorly on nonlinear problems where SVM excels. Conversely, SVM is computationally efficient compared to decision trees but struggles with non-linearly separable data, where decision trees are more flexible and capture nonlinear relationships better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d9d765be95e6cc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Part 2: Classification Metrics\n",
    "---\n",
    "In this part, you will make sense of classification metrics produced by various classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2ad4f31491e50b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The confusion matrix below represents the predictions generated by a classisification model on a small testing dataset.\n",
    "\n",
    "![cnf matrix](https://curriculum-content.s3.amazonaws.com/data-science/images/cnf_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4b5c09376d185ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1) Create a numeric variable `precision` containing the precision of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823529411764706\n"
     ]
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "TP =30\n",
    "FP =4\n",
    "#precision formule\n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "\n",
    "print(precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test confirms that you have created a numeric variable named precision\n",
    "\n",
    "assert isinstance(precision, Number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-09e2fa2bf91d1c95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2) Create a numeric variable `f1score` containing the F-1 score of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bce80c352c6ad99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7894736842105262"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "TP = 30\n",
    "FP = 4\n",
    "FN = 12 \n",
    "precision = TP/(TP+FP)\n",
    "recall = TP /(TP+FN)\n",
    "#calculating F1\n",
    "f1score = 2 * (precision * recall)/ (precision+ recall)\n",
    "f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test confirms that you have created a numeric variable named f1score\n",
    "\n",
    "assert isinstance(f1score, Number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c9611b7378f9cd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The ROC curves below were calculated for three different models applied to one dataset.\n",
    "\n",
    "1. Only Age was used as a feature in the model\n",
    "2. Only Estimated Salary was used as a feature in the model\n",
    "3. All features were used in the model\n",
    "\n",
    "![roc](https://curriculum-content.s3.amazonaws.com/data-science/images/many_roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b2fccd135d7bd12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3) Short Answer: Identify the best ROC curve in the above graph and explain why it is the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "The best ROC curve is the curve of the model using all features 'pink line'. This model has the highest area under curve value compared to others, meaning the true positive rate is higher and the false positive rate is lower. This shows that the model is more accurate and effective in classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a2e4b682abfc6ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the following cells to load a sample dataset, run a classification model on it, and perform some EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e7642482fd78eb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifier has an accuracy score of 0.956.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "network_df = pickle.load(open('sample_network_data.pkl', 'rb'))\n",
    "\n",
    "# partion features and target \n",
    "X = network_df.drop('Purchased', axis=1)\n",
    "y = network_df['Purchased']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2019)\n",
    "\n",
    "# scale features\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "# build classifier\n",
    "model = LogisticRegression(C=1e5, solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# get the accuracy score\n",
    "print(f'The classifier has an accuracy score of {round(accuracy_score(y_test, y_test_pred), 3)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e21cfbd2172b791a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Purchased\n",
       "0    257\n",
       "1     13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3dee6c580108f26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.4) Short Answer: Explain how the distribution of `y` shown above could explain the high accuracy score of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "\n",
    "y distribution shows that there is a large imbalance between classes. Even if the model mostly predicts 0, the correct classification rate will appear high. However, this means that the model actually does not describe the minority class, 1, well enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15288334b184b850",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.5) Short Answer: What is one method you could use to improve your model to address the issue discovered in Question 3.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "\n",
    "Hyperparameter tuning or other model optimization techniques can be used to achieve better performance. Or We can use ensemble methods. Random Forest can creates multiple decision trees and performs classification based on the results of each tree. It can perform better on imbalanced data as different data samples are sent to each tree to better learn the majority and minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bdb41dda25eb6b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Part 3: Decision Trees\n",
    "---\n",
    "In this part, you will use decision trees to fit a classification model to a wine dataset. The data contain the results of a chemical analysis of wines grown in one region in Italy using three different cultivars (grape types). There are thirteen features from the measurements taken, and the wines are classified by cultivar in the `target` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15de0bc4280a2aac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Relevant imports \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the data \n",
    "wine = load_wine()\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X = pd.DataFrame(X, columns=wine.feature_names)\n",
    "y = pd.Series(y)\n",
    "y.name = 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-561128e9ee6b0299",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.1) Use `train_test_split()` to split `X` and `y` data between training sets (`X_train` and `y_train`) and test sets (`X_test` and `y_test`), with `random_state=1`. Evenly split the data between train and test (50/50).\n",
    "\n",
    "Do not alter `X` or `y` before performing the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0be055a675c0a674",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These tests confirm that you have created DataFrames named X_train, X_test and Series named y_train, and y_test\n",
    "\n",
    "assert type(X_train) == pd.DataFrame\n",
    "assert type(X_test) == pd.DataFrame\n",
    "assert type(y_train) == pd.Series\n",
    "assert type(y_test) == pd.Series\n",
    "\n",
    "# These tests confirm that you have split the data evenly between train and test sets\n",
    "\n",
    "assert X_train.shape[0] == X_test.shape[0]\n",
    "assert y_train.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (89, 13)\n",
      "X_test shape: (89, 13)\n",
      "y_train shape: (89,)\n",
      "y_test shape: (89,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eac2fc7be9725bf0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.2) Create an untuned decision tree classifier `wine_dt` with `random_state=1` and fit it using `X_train` and `y_train`. \n",
    "\n",
    "Use parameter defaults for your classifier. You must use the Scikit-learn DecisionTreeClassifier (docs [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28bca1a3b0de0dd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "wine_dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Fit\n",
    "wine_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test confirms that you have created a DecisionTreeClassifier named wine_dt\n",
    "\n",
    "assert type(wine_dt) == DecisionTreeClassifier\n",
    "\n",
    "# This test confirms that you have set random_state to 1\n",
    "\n",
    "assert wine_dt.get_params()['random_state'] == 1\n",
    "\n",
    "# This test confirms that wine_dt has been fit\n",
    "\n",
    "sklearn.utils.validation.check_is_fitted(wine_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-55b417dc67abb7c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.3) Create an array `y_pred` generated by using `wine_dt` to make predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 0, 2, 1, 0, 2, 1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 0, 0, 2,\n",
       "       2, 0, 1, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 2, 2, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 2, 2, 0, 1, 0, 0, 1, 2, 1, 1, 0, 2, 1, 2, 0,\n",
       "       1, 0, 1, 0, 2, 1, 2, 2, 1, 1, 1, 2, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0,\n",
       "       1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "y_pred = wine_dt.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test confirms that you have created an array-like object named y_pred\n",
    "\n",
    "assert type(np.asarray(y_pred)) == np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-536526728a8066e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.4) Create a numeric variable `wine_dt_acc` containing the accuracy score for your predictions. \n",
    "\n",
    "Hint: You can use the `sklearn.metrics` module or the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67272706fb08c3bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8764044943820225"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "wine_dt_acc = accuracy_score(y_test, y_pred)\n",
    "wine_dt_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test confirms that you have created a numeric variable named wine_dt_acc\n",
    "\n",
    "assert isinstance(wine_dt_acc, Number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-266fbd755dbbb4c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.5) Short Answer: Based on the accuracy score, does the model seem to be performing well or does it have substantial performance issues? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "The model performed reasonably well, but not perfectly. An accuracy rate of 87.6% indicates that the model was successful in most of its predictions, but made some errors.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
